{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Unit 5 Live Session </h1>\n",
    "<h3> W203 Instructional Team </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://imgflip.com/i/2bljj9\"><img src=\"https://i.imgflip.com/2bljj9.jpg\" title=\"made at imgflip.com\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Class Announcements </h3>\n",
    "1. TA Responsibilities\n",
    "2. HW 5\n",
    "3. ....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 1 Discrete Random Variables Review </h3>\n",
    "\n",
    "Last time we discussed random variables in a general way by emphasizing that functions of  random variable are themselves random variables,\n",
    "\n",
    "** Marginal Distributions: ** \n",
    "\n",
    "Suppose that a random variable $X$ takes on $N_X$ values, and $Z = f(X)$ for some function $f$ which results in Z taking on $N_Z$ values. Our previous discussion allows us to write the following\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E(Z) = \\sum_{i=1}^{N_Z}Z_i \\cdot P(Z = Z_i) \\hspace{1cm} \\text{ and also } \\hspace{1cm} E(Z) = E(f(X)) = \\sum_{i=1}^{N_x}f(X_i) \\cdot P(X = X_i) \n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "** Binomial Distribution: **\n",
    "\n",
    "Suppose we are repeating an experiment having only two outcomes $ \\{\\text{success},\\text{failure}\\}$, $N$ times in such are way that their outcomes are independent where $p = P(\\text{success})$, then \n",
    "\n",
    "$$ P(\\text{ k successes in N trials }) = {{N}\\choose{k}}p^k(1-p)^{N-k} \\;\\;\\; \\text{ where } \\;\\;\\;{{N}\\choose{k}} = \\frac{N!}{(N-k)!k!}  $$\n",
    "\n",
    "\n",
    "** Joint Distributions: **\n",
    "\n",
    "Now, suppose we have two random variables $X$ and $Y$ which take on $N_X$ and $N_Y$ values respectively, We calculate some of the characteristics of their joint distributions as follows. \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "E(Y|X = X_i) &= \\sum_{j = 1}^{N_Y} Y_j \\cdot P(Y = Y_j | X = X_i)\\\\[3pt]\n",
    "E(XY) &= \\sum_{i=1}^{N_X} \\sum_{j=1}^{N_Y} X_iY_j\\cdot P(X_i,Y_j) \\\\[5pt]\n",
    "\\text{cov}(X,Y) & = E(XY) - E(X)E(Y)\n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "** Law of iterated expectations **\n",
    "\n",
    "One thing that will make your life immensley easier is the law of iterated expectations \n",
    "\n",
    "$$ E(Y) = E[E(Y|X)] $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3> 2 Joint Distribution Practice: Professorial Mistakes (Discrete RVs) </h3>\n",
    "\n",
    "In a live session, the number of questions that students ask is a random variable, $X$.  $X$ takes on three values: $\\{0, 1, 2\\}$ each with probability $1/3$.  Every time a student asks a question, Professor Paul Laskowski answers incorrectly with probability $1/4$, independently of other questions.  Let $Y$ be the random variable representing the number of incorrect responses in the live session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** How can we characterize the random variable $X$? First define the probability mass function. Then compute the expectation. What is a real-world setting in which you might encounter a discrete RV of this sort?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** How should we characterize the random variable $Y$? Note that the pmf of Y has been defined for us in terms of the outcomes of $X$. Describe a real-world setting in which you could encounter jointly distributed RVs of this sort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3** Compute the expectation of $Y$, conditional on $X$, $E(Y|X)$ First remind yourself of the definition of conditional expectation. What does this expression tell us?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4** Using the law of iterated expectations, compute $E(Y)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5** Describe the joint probability distribution of $X$ and $Y$.  (You may find it easiest to use a table). For an introduction to joint pdfs of discrete RVs, see example 5.1 in Devore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6** Compute the expectation of the product of $X$ and $Y$, $E(XY)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7** Using the previous result, compute $\\text{cov}(X,Y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3 Joint Distribution Practice:  Triangular Regions </h3>\n",
    "\n",
    "Continuous random variables $X$ and $Y$ have a joint distribution with probability density function,\n",
    "\n",
    "$$ f(X,Y) = \\begin{cases}\n",
    "1 & \\text{ if } \\;0 < Y < 1 \\;\\text{ and } \\; a \\cdot Y < X < a \\cdot Y + 1 \\\\\n",
    "0 & \\text{ if otherwise.}\n",
    "\\end{cases} $$\n",
    "\n",
    "where $a$ is a constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** Note: For a problem like this, we have to characterize the RVs jointly before we can talk about them separately. A real-world situation that we might want to model using jointly distributed continuous random variables like these could be the results of two service requests, where the initial service request $Y$ is typically shorter,and the follow up request, $X$, is typically longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** Choose 2 example values for $a$ and draw a graph of the region for which $X$ and $Y$ have positive probability density. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3** Derive the marginal distribution of $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4** As a slight variation on the previous part, compute $E(XY | Y)$.  Note that since we're conditioning on Y, the Y inside the expectation is just a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5** Derive $\\text{cov}(X,Y)$.  Hint: an nice way to do this is to use the law of iterated expectations.  Write down the definition of covariance, then break the expectation up into two expectations.  The inner expecation should be conditional on Y, and the outer expectation should be unconditional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.6** Check what $cov(X,Y)$ equals when $a = 0$.  What is $cov(X,Y)$ when $a = -1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.7** Choose an example value for a, then use R to simulate 100 draws from the given joint distribution and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4 Discussion on Models </h3>\n",
    "\n",
    "<center>![title](dog_model.png)</center>\n",
    "\n",
    "Reading: \n",
    "\n",
    "The End of Theory: The Data Deluge Makes the Scientific Method Obsolete\n",
    "\\newline\n",
    "<http://archive.wired.com/science/discoveries/magazine/16-07/pb_theory>\n",
    "\n",
    "*All models are wrong, but some are useful.* - George Box, Statistician\n",
    "\n",
    "*All models are wrong, and you can increasingly succeed without them.* - Peter Norvig, Google's Research Director\n",
    "\n",
    "Setting aside the over-dramatic tone of Anderson's piece, there are many elements of his argument worth discussing:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1** Are the fancy algorithms Anderson points to actually model-free?  For example, is the page rank algorithm model-free?  What is a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2** What is the real difference between parametric statistics and learning algorithms that originate in a computer science tradition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3** In part, Anderson argues that we don't need models that are understandable by humans.  When is it enough to have a model that \"fits\" the data well, and when do we really need it to be human-readable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4** When does context matter in data science? Is domain knowledge important or can we just rely on the numbers in a data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.5** Anderson seems to imply that we don’t need to test hypotheses today.  Are hypotheses still relevant to data science, or should our focus be on estimating effect sizes, or using our models to classify and predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
