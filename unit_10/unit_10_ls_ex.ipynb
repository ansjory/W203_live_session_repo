{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Unit 10 Live Session </center> </h2>\n",
    "<h4> W203 Instructional Team </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Linear Regression\n",
    "<center>![title](regression.png)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Announcements\n",
    "1. Announcement 1\n",
    "2. Announcement 2\n",
    "3. Announcement 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.0  Simple Linear Regression\n",
    "\n",
    "Suppose we have data, represented by ($X_1$, $Y_1$), . . . , ($X_n$, $Y_n$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1.1 **  Let $u_i$ be an error term, write a simple regression model for the $i^{th}$ observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1.2 ** In words, what do the statistical errors $u_i$ represent ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1.3 ** What assumption(s) are needed in order to interpret $\\beta_0 + \\beta_1X_i$ as a conditional expectation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1.4 ** Do we want the residuals $\\hat{u}_i = Y_i - \\hat{\\beta}_0 + \\hat{\\beta}_1X_i$ to be small in magnitude? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 1.5 ** To define a regression line, is it sufficient to require $\\sum \\hat{u_i} = 0$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0 Properties of residuals\n",
    "\n",
    "We derive our estimator for $\\beta_0$ and $\\beta_1$  by setting our sample moments equal to their theoretical values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2.1 ** What are the implications of the following properties?\n",
    "1. $n^{-1}\\sum_{i=1}^n \\hat{u_i} = 0$.\n",
    "\n",
    "2. $n^{-1}\\sum_{i=1}^n X_i \\hat{u_i} = 0$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2.2 ** How many different lines through the X-Y plane would fulfill these two conditions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.0 Regression in R\n",
    "When a linear pattern is evident from a scatter plot, the relationship between the two variables is often modeled with a straight line. This line is expressed in a linear model between the response (or dependent) variable and the predictor (or independent) variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are useful for running a linear regression in R.\n",
    "\n",
    "- Fitting a model: model <- lm(y ~ x)\n",
    "- Coefficients: model\\$coef or coef(model)\n",
    "- Fitted values: model$fitted or fitted(model)\n",
    "- Residuals: model\\$resid or resid(model)\n",
    "\n",
    "Install and load the BSDA pacakge using the commands install.packages(\"BSDA\") and library(BSDA),respectively.\n",
    "\n",
    "We are interested in using the GPA data frame, which we can attach using the command: attach(Gpa), to investigate the impact of high school GPA on college GPA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can find the least square regression line, we need to determine the explanatory and response varibles. Define 2 new variables in R, x and Y, and assign the explanatory and response variables from the dataset, respectively, and conduct a cursory analysis of the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.1 ** Create a scatterplot of CollGPA versus HSGPA and find the correlation between the two variables. What can we infer from the correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.2 ** Create a scatterplot of CollGPA versus HSGPA and find the correlation between the two variables. What can we infer from the correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.3 ** Now that we know a few things about the data, we want to find a line that best represents the relationship between the variables. In other words, we want to draw a slope that comes closest to describing the data.\n",
    "\n",
    "Characterize the equation mathematically. Find the least squares estimates of $\\beta_0$ and $\\beta_1$. Corresponding to the model. \n",
    "$$ CollGPA = \\beta_0 + \\beta_1\\cdot HSGPA$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note: ** To perform the least square regression is R we can use the lm command. If you are interested use the help(lm) command to learn the different options for using this function. To relationship between the variables is defined in the lm command using a tilde (\"~\") between the vector containing the response variable and the vector containing the explanatory variable: lm(Y ~ x).\n",
    "\n",
    "If you would like to know what else is stored in the variable you can use the attributes command: attributes(). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.3 ** Find the least squares estimates of $\\beta_0$ and $\\beta_1$ using the R\n",
    "function lm()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.4 ** abline() adds one or more straight lines to the current plot. The arguments to abline() are a=b0 and b=b1. Add the least squares line to the scatterplot created in 1 using the R function abline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.5 ** Compute the sample correlation between $X$ and $\\hat{u}_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.0 OLS Goodness of Fit\n",
    "\n",
    "When building regression models, \"goodness-of-fit\" explains how closely our model of the data (i.e. the predictor variables) fits the outcome data. In other words, how much of the variation in an outcome can we explain with a particular model? \n",
    "\n",
    "** R-Squared ** is a measure commonly used for assessing model fit.  It can be understood as the proportion of variance in the outcome that can be accounted for by the model.\n",
    "\n",
    "Looking at our simple bivariate model, we can extract R-squared as a measure of model fit in a number of ways. The easiest is simply to extract it from the lm object using summary(model)\\$r.squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.712206122414635"
      ],
      "text/latex": [
       "0.712206122414635"
      ],
      "text/markdown": [
       "0.712206122414635"
      ],
      "text/plain": [
       "[1] 0.7122061"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(model)$r.squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: We normally discourage students from using the summary command with lm objects.  The reason, as we will see later, is that summary makes a strong assumption called homoskedasticity, which is usually not justified.  However, it is ok to use the command in order to extract R-squared.\n",
    "\n",
    "But we can also calculate R-squared from our data in a number of ways. Take a couple of minutes to manually calculate R-squared.\n",
    "\n",
    "1. By squaring the correlation between X and Y.\n",
    "2. By taking the ratio of the variance of the fitted values to the variance of Y.\n",
    "3. By weighting the slope coefficient: $R^2 = \\beta_1^2 \\frac{var(X)}{var(Y)}$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.0 Adjusted R Square\n",
    "\n",
    "The \"Adjusted R-squared\" is commonly used in place of the \"regular\" R-squared, which is sensitive to the number of independent variables in the model. In other words, as we put more variables into the model, R-squared increases even if those variables are unrelated to the outcome. \n",
    "\n",
    "Adjusted R-squared attempts to correct for this by deflating R-squared by the expected amount of increase from including irrelevant additional predictors. \n",
    "\n",
    "We can see this property of R-squared and Adjusted R-squared by adding a completely random variables unrelated to our other covariates or the outcome into our model and examine the impact on R-squared and Adjusted R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 5.1 ** Add this variable to your simple regression model, creating a new lm object, then observe what happens to R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 5.2 ** Now extract the adjusted R-squared from both models using lm\\$adj.r.squared.  It may also go down, but by less than regular r-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.0 OLS: Issues to be Aware of\n",
    "\n",
    "Unfortunately, the pitfalls of applying least squares are not often well understood by many of the people who attempt to apply it. What follows is a list of some of the biggest problems with using least squares regression in practice, along with some brief comments about how these problems may be mitigated or avoided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Outliers: **  Least squares regression can perform very badly when some points in the training data have excessively large or small values for the dependent variable compared to the rest of the training data. The reason for this is that since the least squares method is concerned with minimizing the sum of the squared error, any training point that has a dependent value that differs a lot from the rest of the data can have a disproportionately large effect on the resulting constants that are being solved for.\n",
    "\n",
    "**WARNING: Do not ever remove an observation just because it's an outlier.**\n",
    "\n",
    "** 6.1 ** Returning to our example, let's add an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 6.2 ** We can see the outlier pulls the correlation off a lot.  Let's see what it does to the linear model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 6.3 ** Let's see that scatterplot again with our new regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING: Do not ever remove an observation just because it's an outlier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Non-Linearities ** All linear regression methods (including, of course, least squares regression), suffer from the major drawback that in reality most systems are not linear.\n",
    "\n",
    "Let's take another dataset that is clearly non-linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's definitely a relationship here, but we will need to do a transformation prior to OLS."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
