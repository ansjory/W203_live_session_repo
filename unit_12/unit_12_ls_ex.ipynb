{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Unit 12 Live Session </center> </h2>\n",
    "<h4> W203 Instructional Team </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multivariate Linear Regression, Inference\n",
    "<center>![title](hypothesis_1.jpg)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Announcements\n",
    "1. Announcement 1\n",
    "2. Announcement 2\n",
    "3. Announcement 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.0 Useful functions in R:\n",
    "\n",
    "Code    |    Function \n",
    "--------------------------|---------------------------------------------------------------------\n",
    "coefficients(fit) | Extract model coefficients\n",
    "fitted(fit)       | Extract predicted values\n",
    "residuals(fit)    | Extract resduals\n",
    "vcovHC(fit)       | Extract heteroskedasticity-robust covariance matrix\n",
    "coeftest(fit, vcov = vcovHC) | Conduct hypothesis test with heteroskedasticity-robust standard errors\n",
    "confint(fit, level=0.95)| Calculate non-robust CIs for model parameters (at 95%) \n",
    "\n",
    "**Note:** For heteroskedasticity-robust confidence intervals, get the variance of each coefficient from vcovHC, take the square root to get the standard error, get the proper t critical values from qt, and construct manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0 Variance of OLS Estimators\n",
    "Recall (one of) the expression(s) for the variance of each OLS slope coefficient:\n",
    "\n",
    "$$var(\\hat{\\beta_j}) = \\frac{\\sigma^2}{SST_j (1-R_j^2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** A Crappy Analogy: ** Sometimes when you are (re)learning linear regression it is helpful to think in terms of analogies, so here goes... \n",
    "\n",
    "Imagine that you have gone to popular club late on a friday night with two friends Alex and Bob. You find yourself a table and sit down close to the dance floor. This would be the perfect opportunity for you to try out the new shopping cart dance move you have been practicing in front of your dog all week but its been a long night and the three you only want to talk.\n",
    "\n",
    "Your conversation starts but you are having trouble following the conversation for two distinct reasons. \n",
    "\n",
    "1. You are having trouble distiguishiing the voices of your friends from the overall noise in the club. \n",
    "\n",
    "2. The lights are flashing in such a way that the faces of your friends are in total darkness most of the time, as a result you even when you can distinguish between the background club noises and your friends you ar having trouble distinguishing Alex's voice from Bob's. \n",
    "\n",
    "The background noise in the club is analagous to the error term $u$ of the regression,\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + u $$\n",
    "\n",
    "$u$ is the variation in your outcome $Y$ (sound in the club) which is just uninteresting noise. \n",
    "\n",
    "In this context regression is an attempt to distinguish the variation in $Y$ (sound in the club) due to regressors $X_1$ and $X_2$, (Alex and Bob respectively) from the noise $u$ (Boots and Pants, \"it's getting hot in here so ....\", \" Yeah, yeah, ... \", \" oh my good she is such a ....\"). \n",
    "\n",
    "Regression is also an attempt to distinguish the variation in $Y$ (sound in the club) due to $X_1$ (Alex) from the variation in $Y$ due to $X_2$ (Bob)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2.1 ** Why is it desirable to have a small variance for each estimated coefficient? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2.2 **  For each component of this equation, explain (1) what it means, and (2) why it moves the standard error of $\\beta_j$ up or down.\n",
    "\n",
    "* $\\sigma^2$\n",
    "* $SST_j$\n",
    "* $R_j^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Component 3 has a special name: the Variance Inflation Factor.  You can find the variance inflation factor for each variable in a linear model using the vif function in the car package.  Interpreting VIFs depends very much on context, but a VIF of 10 would usually be considered very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the variance of each coefficient in R, we would typically get the diagonal elements of the robust covariance matrix, diag(vcovHC(model))\n",
    "\n",
    "To get the standard error of a coefficient, take the square root of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.0 R Exercise\n",
    "\n",
    "In this analysis, we will use the mtcars dataset which is a dataset that was extracted from the 1974 Motor Trend US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models). The dataset is automatically available when you start R.  For more information about the dataset, use the R command: help(mtcars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Please cite as: \n",
      "\n",
      " Hlavac, Marek (2018). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n",
      " R package version 5.2.1. https://CRAN.R-project.org/package=stargazer \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# conda install -c r r-car\n",
    "# conda install -c r r-lmtest\n",
    "# conda install -c r r-sandwich\n",
    "# conda install -c r r-stargazer\n",
    "library(car)\n",
    "library(lmtest)\n",
    "library(sandwich)\n",
    "library(stargazer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.1 ** Using the mtcars data, run a multiple linear regression to find the effect of displacement (disp), gross horsepower (hp), weight (wt), and rear axle ratio (drat) on the miles per gallon (mpg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.2: ** For ** each ** of the following 6 CLM assumptions, assess whether the assumption holds.  Where possible, demonstrate multiple ways of assessing an assumption.  When an assumption appears violated, state what steps you would take in response.\n",
    "\n",
    "1. Linear population model\n",
    "2. Random Sampling\n",
    "3. No perfect multicollinearity\n",
    "4. Zero-conditional mean\n",
    "5. Homoskedasticity\n",
    "6. Normality of Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.3 ** In addition to the above, assess to what extent (imperfect) multicollinearity is affecting your inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.4 ** Interpret your slope coefficients, and note which ones are significantly different from zero.  Whether or not you detected heteroskedasticity above, be conservative in this step and use robust standard errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5** How does the log transform affect which CLM assumptions hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.6 ** Which model has a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.7 ** (As time allows) Report the results of both models in a nicely formatted regression table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.0 More about Multicollinearity\n",
    "\n",
    "A common problem with multivariate regression is collinearity.\n",
    "If two or more predictor variables are highly correlated, and they are both entered into a regression model, it increases the standard error of each one and you get very unstable estimates of the slope. We usually assess the collinearity by variance inflation factor (VIF). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Ways to Detect Multicollinearity \n",
    "\n",
    "We begin by regressing a particular independent variable on all other independent variables.\n",
    "\n",
    "1. As the squared correlation (r2) increases toward 1.0, the magnitude of potential problems associated with multicollinearity increases correspondingly. \n",
    "\n",
    "2. Tolerance (1-R2) One minus the squared multiple correlation of a given IV from other Ivs in the equation. Tolerance values of 0.10 or less Indicate that there may be serious multicollinearity. \n",
    "\n",
    "3. The Variance Inflation Factor [VIF=1/(1-R2)] VIF Is the reciprocal of the Tolerance. Any VIF of 10 or more provides evidence of serious multicollinearity. \n",
    "\n",
    "4. Condition Number (k) The square root of the ratio of the largest eigenvalue to the smallest eigenvalue. k of 30 or larger indicate that there may be serious multicollinearity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
